{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzI9sn+Wc2ssqIN/fkZeCO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertobiero/selfAttentionModule/blob/main/SelfAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_iPnepooUVc",
        "outputId": "daac3493-49d8-4408-ecb9-5fbb6aa4a83e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q_journey tensor([0.4306, 1.4551])\n",
            "K_journey tensor([0.4433, 1.1419])\n",
            "V_journey tensor([0.3951, 1.0037])\n",
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
            "attention weights tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
            "Value vector tensor([[0.1855, 0.8812],\n",
            "        [0.3951, 1.0037],\n",
            "        [0.3879, 0.9831],\n",
            "        [0.2393, 0.5493],\n",
            "        [0.1492, 0.3346],\n",
            "        [0.3221, 0.7863]])\n",
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "x_2 = inputs[1] # second input element\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2 # the output embedding size, d=2\n",
        "\n",
        "\n",
        "class selfAttention(torch.nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__(). # run init method from parent class torch.nn.module\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    Q = self.W_query(x). # linear projection of embedding x to query space\n",
        "    K = self.W_key(x)\n",
        "    V = self.W_value(x)\n",
        "\n",
        "    attn_scores = Q @ K.T # compute attention scores\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n",
        "\n",
        "torch.manual_seed(789).  # randomly initialise W-q, W_k, W_v\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-hozSF0Qob57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}